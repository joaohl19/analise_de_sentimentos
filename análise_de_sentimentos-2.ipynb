{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "rhZFolpBjFtL",
        "cbhji8k486sI",
        "4hWn-p4l-afF",
        "EPUrkxTc-3he",
        "V8EI9ytUSl_i",
        "yykJwQCtpa_R",
        "9s3flP5TSsuv",
        "q6eeVTLVMDLG"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#RNN para Classificação de sequências"
      ],
      "metadata": {
        "id": "rhZFolpBjFtL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Base de dados utilizada:** https://huggingface.co/datasets/stanfordnlp/imdb"
      ],
      "metadata": {
        "id": "_22rc4uzv2Jm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "cbhji8k486sI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import re\n",
        "import pandas as pd\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch import nn\n",
        "from datasets import load_dataset, concatenate_datasets, DatasetDict\n",
        "from torch.utils.data import  Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from collections import Counter\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from typing import List, Tuple"
      ],
      "metadata": {
        "id": "XQzT4v3y8_M6"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extração e Preprocessamento"
      ],
      "metadata": {
        "id": "4hWn-p4l-afF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer:\n",
        "  def __init__(self):\n",
        "    self.pattern = re.compile(r'\\w+|[^\\w\\s]') # Returns an object of re.Pattern class\n",
        "\n",
        "  def __call__(self, text) -> List[str]:\n",
        "    return self.pattern.findall(text.lower())"
      ],
      "metadata": {
        "id": "NelqgcCKfUe_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocabulary:\n",
        "  def __init__(self, min_freq=1):\n",
        "    self.min_freq = min_freq\n",
        "    self.pad_idx = 1\n",
        "    self.to_idx = {}\n",
        "    self.to_token = {}\n",
        "    self.specials = ['<unk>', '<pad>']\n",
        "    for i, special in enumerate(self.specials):\n",
        "      self.to_idx[special] = i\n",
        "      self.to_token[i] = special\n",
        "\n",
        "  def build_vocabulary(self, texts, tokenizer):\n",
        "    counter = Counter() # Contador para hashtable objects\n",
        "\n",
        "    for text in texts:\n",
        "      tokens = tokenizer(text) # Divide o texto 'text' em tokens\n",
        "      counter.update(tokens) # Atualiza a frequência dos tokens encontrados na variável tokens\n",
        "\n",
        "    idx = len(self.specials)\n",
        "    for token, freq in counter.items(): # Construção do vocabulário\n",
        "      if freq >= self.min_freq:\n",
        "        self.to_idx[token] = idx\n",
        "        self.to_token[idx] = token\n",
        "        idx += 1\n",
        "\n",
        "    print(f\"Vocabulário construído: {len(self.to_idx)} tokens.\")\n",
        "\n",
        "  def vocab_to_idx(self, text, tokenizer):\n",
        "    tokens = tokenizer(text)\n",
        "    return [self.to_idx.get(token, self.to_idx['<unk>']) for token in tokens]"
      ],
      "metadata": {
        "id": "rJZKVflcrNhb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(path:str = \"stanfordnlp/imdb\"):\n",
        "  ds = load_dataset(path)\n",
        "  return ds"
      ],
      "metadata": {
        "id": "yhxabnBsFKQP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = get_data()"
      ],
      "metadata": {
        "id": "P4wmReKAGhAB",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data(ds, random_state:int = 42) -> Tuple:\n",
        "  # Divisão em datasets de treino, validação e teste(70%, 15%, 15%)\n",
        "  extra_train_ds = ds['test']\n",
        "  split_ds = extra_train_ds.train_test_split(\n",
        "      test_size=0.6,\n",
        "      shuffle=True,\n",
        "      seed=random_state,\n",
        "      stratify_by_column='label'\n",
        "  )\n",
        "  train_ds = concatenate_datasets([ds['train'], split_ds['train']])\n",
        "\n",
        "  val_test_ds = split_ds['test'].train_test_split(\n",
        "      test_size=0.5,\n",
        "      shuffle=True,\n",
        "      seed=random_state,\n",
        "      stratify_by_column='label'\n",
        "  )\n",
        "  final_ds = DatasetDict({\n",
        "      'train' : train_ds,\n",
        "      'validation' : val_test_ds['train'],\n",
        "      'test' : val_test_ds['test']\n",
        "  })\n",
        "\n",
        "  train_ds = final_ds['train']\n",
        "  val_ds = final_ds['validation']\n",
        "  test_ds = final_ds['test']\n",
        "\n",
        "  return train_ds, val_ds, test_ds"
      ],
      "metadata": {
        "id": "KjAYxPbwEykJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds, val_ds, test_ds = split_data(ds)"
      ],
      "metadata": {
        "id": "ZrvP53YjG3XS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_data(train_ds, val_ds, test_ds, batch_size:int):\n",
        "  # Criando o vocabulário\n",
        "  tokenizer = Tokenizer()\n",
        "  vocab = Vocabulary(min_freq=5)\n",
        "\n",
        "  vocab.build_vocabulary(train_ds['text'], tokenizer)\n",
        "\n",
        "  # Função que processa o batch retornado do DataLoader\n",
        "  def collate_fn(batch):\n",
        "    labels = []\n",
        "    texts = []\n",
        "    lengths = []\n",
        "    for sample in batch:\n",
        "      labels.append(sample['label'])\n",
        "\n",
        "      numerical_tokens = vocab.vocab_to_idx(sample['text'], tokenizer)\n",
        "      if len(numerical_tokens) > 600: # Tamanho máximo da sequência\n",
        "        numerical_tokens = numerical_tokens[:600]\n",
        "\n",
        "      lengths.append(len(numerical_tokens))\n",
        "      text_tensor = torch.tensor(numerical_tokens, dtype=torch.long)\n",
        "      texts.append(text_tensor)\n",
        "\n",
        "    texts_tensor = pad_sequence(texts, batch_first=True, padding_value=vocab.pad_idx)\n",
        "    labels_tensor = torch.tensor(labels, dtype=torch.float32)\n",
        "    lengths_tensor = torch.tensor(lengths, dtype=torch.long)\n",
        "\n",
        "    return texts_tensor, labels_tensor, lengths_tensor\n",
        "\n",
        "  train_dataloader = DataLoader(train_ds, batch_size=batch_size, num_workers=2, shuffle=True, collate_fn=collate_fn, pin_memory=True)\n",
        "  val_dataloader = DataLoader(val_ds, batch_size=batch_size, num_workers=2, shuffle=False, collate_fn=collate_fn, pin_memory=True)\n",
        "  test_dataloader = DataLoader(test_ds, batch_size=batch_size, num_workers=2, shuffle=False, collate_fn=collate_fn, pin_memory=True)\n",
        "\n",
        "  return train_dataloader, val_dataloader, test_dataloader, vocab"
      ],
      "metadata": {
        "id": "abUut9D0AlbW"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embeddings pré-treinados"
      ],
      "metadata": {
        "id": "EPUrkxTc-3he"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download dos embeddings pré-treinados\n",
        "!wget https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ],
      "metadata": {
        "id": "odB36c7f_uyW",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_glove_matrix(vocab, embed_dim, glove_path):\n",
        "    embeddings_index = {}\n",
        "    try:\n",
        "        with open(glove_path, 'r', encoding='utf8') as f:\n",
        "            for line in f:\n",
        "                values = line.split()\n",
        "                word = values[0]\n",
        "                coefs = np.asarray(values[1:], dtype='float32')\n",
        "                embeddings_index[word] = coefs\n",
        "    except FileNotFoundError:\n",
        "        print(f\"ERRO: Arquivo {glove_path} não encontrado!\")\n",
        "        return None\n",
        "\n",
        "    # Matriz inicializada aleatoriamente (para palavras que não estão no GloVe)\n",
        "    weights_matrix = np.random.normal(scale=0.6, size=(len(vocab.to_idx), embed_dim))\n",
        "\n",
        "    hits = 0\n",
        "    misses = 0\n",
        "\n",
        "    # Alinha ao vocabulário\n",
        "    for word, i in vocab.to_idx.items():\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            weights_matrix[i] = embedding_vector\n",
        "            hits += 1\n",
        "        else:\n",
        "            if word == '<pad>':\n",
        "                weights_matrix[i] = np.zeros(embed_dim)\n",
        "            misses += 1\n",
        "\n",
        "    print(f\"Embeddings carregados. Hits: {hits}, Misses: {misses}\")\n",
        "    return torch.tensor(weights_matrix, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "7EUdQfOS-8Cr"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parâmetros"
      ],
      "metadata": {
        "id": "V8EI9ytUSl_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "embed_dim = 100\n",
        "hidden_dim = 64\n",
        "num_layers = 2\n",
        "output_dim = 1\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "dropout_rate = 0.5"
      ],
      "metadata": {
        "id": "JJvvdZHzDdpk"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GRU Class"
      ],
      "metadata": {
        "id": "yykJwQCtpa_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GRU(nn.Module):\n",
        "  def __init__(self, vocab_size, pad_idx, num_layers,\n",
        "               input_dim, hidden_dim, dropout_rate, output_dim=1,pretrained_embeddings=None):\n",
        "    super().__init__()\n",
        "\n",
        "    # Treinar os embeddings(ou não)\n",
        "    if pretrained_embeddings is not None:\n",
        "        self.embedding = nn.Embedding.from_pretrained(\n",
        "            pretrained_embeddings,\n",
        "            padding_idx=pad_idx\n",
        "        )\n",
        "    else:\n",
        "        self.embedding = nn.Embedding(vocab_size, input_dim, padding_idx=pad_idx)\n",
        "\n",
        "    # Rede GRU\n",
        "    self.gru = nn.GRU(\n",
        "        input_size=input_dim,\n",
        "        hidden_size=hidden_dim,\n",
        "        num_layers=num_layers, # posicionar 'num_layers' GRUs, uma seguida da outra\n",
        "        batch_first=True,\n",
        "        bidirectional=True\n",
        "    )\n",
        "\n",
        "    # Camada linear final para previsão(0 ou 1), concatenando a previsão de cada direção da GRU\n",
        "    self.fc = nn.Linear(2*hidden_dim, output_dim)\n",
        "\n",
        "    # Camada de Dropout\n",
        "    self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "  def forward(self, text, text_lengths):\n",
        "\n",
        "    embedded = self.dropout(self.embedding(text))\n",
        "\n",
        "    if isinstance(text_lengths, list):\n",
        "      text_lengths = torch.Tensor(text_lengths)\n",
        "\n",
        "    text_lengths_cpu = text_lengths.cpu()\n",
        "    packed_embedded = nn.utils.rnn.pack_padded_sequence(\n",
        "        embedded, text_lengths_cpu, batch_first=True, enforce_sorted=False\n",
        "    ).cuda()\n",
        "\n",
        "    output, hidden = self.gru(packed_embedded)\n",
        "\n",
        "    # hidden: [num_layers, batch_size, hidden_dim] --> estado oculto de todas as camadas no último instante de tempo\n",
        "    # output: [seq_length, batch_size, hidden_dim] --> camada de saída\n",
        "    hidden_backward = hidden[-1, :, :] # estado oculto na última camada <--\n",
        "    hidden_forward = hidden[-2, : , :] # estado oculto na última camada -->\n",
        "\n",
        "    hidden_final = torch.cat((hidden_forward, hidden_backward), dim=1)\n",
        "\n",
        "    return self.fc(self.dropout(hidden_final))\n"
      ],
      "metadata": {
        "id": "sUeBrsZMpfDY"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Treinamento"
      ],
      "metadata": {
        "id": "9s3flP5TSsuv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, iterator, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_right = 0\n",
        "    total_samples = 0\n",
        "    for texts, labels, lengths in iterator:\n",
        "        texts = texts.to(device)\n",
        "        labels = labels.to(device)\n",
        "        lengths = lengths.to(device)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        predictions = model(texts, lengths).squeeze(1)\n",
        "        loss = criterion(predictions, labels)\n",
        "        total_loss += loss.item() * labels.size(0)\n",
        "\n",
        "        probs = torch.sigmoid(predictions)\n",
        "        predicted_classes = torch.round(probs)\n",
        "        correct_tensor = (predicted_classes == labels)\n",
        "        total_right += correct_tensor.sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_loss = total_loss / total_samples\n",
        "    avg_acc = total_right / total_samples\n",
        "    return avg_loss, avg_acc"
      ],
      "metadata": {
        "id": "1v75I9ylo2Tu"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_one_epoch(model, iterator, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_right = 0\n",
        "    total_samples = 0\n",
        "    with torch.no_grad():\n",
        "        for texts, labels, lengths in iterator:\n",
        "            texts = texts.to(device)\n",
        "            labels = labels.to(device)\n",
        "            lengths = lengths.to(device)\n",
        "\n",
        "            predictions = model(texts, lengths).squeeze(1)\n",
        "            loss = criterion(predictions, labels)\n",
        "            total_loss += loss.item() * labels.size(0)\n",
        "\n",
        "            probs = torch.sigmoid(predictions)\n",
        "            predicted_classes = torch.round(probs)\n",
        "            correct_tensor = (predicted_classes == labels)\n",
        "            total_right += correct_tensor.sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "    avg_loss = total_loss / total_samples\n",
        "    avg_acc = total_right / total_samples\n",
        "    return avg_loss, avg_acc"
      ],
      "metadata": {
        "id": "s12PQNfNCmUO"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparo dos dados\n",
        "train_loader, val_loader, test_loader, vocab = process_data(train_ds, val_ds, test_ds, batch_size)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Usando: {device}\")\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# --- EXPERIMENTO 1: Treinar Embeddings do Zero ---\n",
        "print(\"\\n--- Treinando Embeddings do Zero ---\")\n",
        "model_scratch = GRU(\n",
        "    len(vocab.to_idx), vocab.pad_idx, num_layers,\n",
        "    embed_dim, hidden_dim, dropout_rate, output_dim, pretrained_embeddings=None\n",
        ").to(device)\n",
        "\n",
        "opt_scratch = optim.Adam(model_scratch.parameters(), lr=learning_rate)\n",
        "crit = nn.BCEWithLogitsLoss()\n",
        "\n",
        "train_avg_losses_scratch = []\n",
        "val_avg_losses_scratch = []\n",
        "train_avg_acc_scratch = []\n",
        "val_avg_acc_scratch = []\n",
        "for epoch in range(epochs):\n",
        "    train_loss, train_acc = train_one_epoch(model_scratch, train_loader, opt_scratch, crit, device)\n",
        "    val_loss, val_acc = validate_one_epoch(model_scratch, val_loader, crit, device)\n",
        "    print(f\"Epoch {epoch+1} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    train_avg_losses_scratch.append(train_loss)\n",
        "    val_avg_losses_scratch.append(val_loss)\n",
        "    train_avg_acc_scratch.append(train_acc)\n",
        "    val_avg_acc_scratch.append(val_acc)\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "print(f\"Tempo de treinamento SEM embeddings pré-treinados: {elapsed_time:.4f} segundos\")\n",
        "\n",
        "\n",
        "# --- EXPERIMENTO 2: Usar Embeddings GloVe ---\n",
        "print(\"\\n--- Usando Embeddings GloVe Pré-Treinados ---\")\n",
        "assert embed_dim == 50 or embed_dim == 100 or embed_dim == 200 or embed_dim == 300\n",
        "glove_weights = load_glove_matrix(vocab, embed_dim, f'/content/glove.6B.{embed_dim}d.txt')\n",
        "\n",
        "crit = nn.BCEWithLogitsLoss()\n",
        "start_time = time.time()\n",
        "\n",
        "model_glove = GRU(\n",
        "    len(vocab.to_idx), vocab.pad_idx, num_layers,\n",
        "    embed_dim, hidden_dim, dropout_rate, output_dim,pretrained_embeddings=glove_weights\n",
        ").to(device)\n",
        "\n",
        "opt_glove = optim.Adam(model_glove.parameters(),lr=learning_rate)\n",
        "\n",
        "train_avg_losses_glove = []\n",
        "val_avg_losses_glove = []\n",
        "train_avg_acc_glove = []\n",
        "val_avg_acc_glove = []\n",
        "for epoch in range(epochs):\n",
        "    train_loss, train_acc = train_one_epoch(model_glove, train_loader, opt_glove, crit, device)\n",
        "    val_loss, val_acc = validate_one_epoch(model_glove, val_loader, crit, device)\n",
        "    print(f\"Epoch {epoch+1} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    train_avg_losses_glove.append(train_loss)\n",
        "    val_avg_losses_glove.append(val_loss)\n",
        "    train_avg_acc_glove.append(train_acc)\n",
        "    val_avg_acc_glove.append(val_acc)\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "print(f\"Tempo de treinamento COM embeddings pré-treinados: {elapsed_time:.4f} segundos\")"
      ],
      "metadata": {
        "id": "8OAW4k9OCtTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_train_val_losses(train_avg_losses, val_avg_losses, num_epochs):\n",
        "  epochs = list(range(1, num_epochs+1))\n",
        "  plt.plot(epochs, train_avg_losses, color='blue', label='Loss do treino')\n",
        "  plt.plot(epochs, val_avg_losses, color='orange', label='Loss da validação')\n",
        "  plt.title('Losses de treino e validação por época de treino')\n",
        "  plt.legend()"
      ],
      "metadata": {
        "id": "8IJrlcuLxR3I"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_train_val_accuracies(train_avg_acc, val_avg_acc, num_epochs):\n",
        "  epochs = list(range(1, num_epochs+1))\n",
        "  plt.plot(epochs, train_avg_acc, color='blue', label='Acurácia do treino')\n",
        "  plt.plot(epochs, val_avg_acc, color='orange', label='Acurácia da validação')\n",
        "  plt.title('Acurácias de treino e validação por época de treino')\n",
        "  plt.legend()"
      ],
      "metadata": {
        "id": "rVZKA-t60rvu"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_train_val_losses(train_avg_losses_scratch, val_avg_losses_scratch, epochs)"
      ],
      "metadata": {
        "id": "oxE8BuA_y_j6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_train_val_accuracies(train_avg_acc_scratch, val_avg_acc_scratch, epochs)"
      ],
      "metadata": {
        "id": "EHBwxXWp06cU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_avg_acc_scratch[-1]"
      ],
      "metadata": {
        "id": "aDmJ7HobUTmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_train_val_losses(train_avg_losses_glove, val_avg_losses_glove, epochs)"
      ],
      "metadata": {
        "id": "p9FgeszbzARd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_train_val_accuracies(train_avg_acc_glove, val_avg_acc_glove, epochs)"
      ],
      "metadata": {
        "id": "hdvRKtJK1D5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_avg_acc_glove[-1]"
      ],
      "metadata": {
        "id": "NacazpkeUZVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Teste"
      ],
      "metadata": {
        "id": "q6eeVTLVMDLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_metrics(y_true, y_pred):\n",
        "  tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "  acc = (tp+tn)/(tp+tn+fp+fn)\n",
        "  recall = tp/(tp+fn)\n",
        "  precision = tp/(tp+fp)\n",
        "  f1 = (2*recall*precision)/(recall+precision)\n",
        "  return {'acc':acc,'precision':precision,'f1-score':f1}"
      ],
      "metadata": {
        "id": "gzr82EVQhQ4Z"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(y_true, y_pred):\n",
        "  cm = confusion_matrix(y_true, y_pred)\n",
        "  group_counts = [f'{value:.0f}' for value in confusion_matrix(y_true, y_pred).ravel()]\n",
        "  group_percentages = [f'{value*100:.2f}%' for value in confusion_matrix(y_true, y_pred).ravel()/np.sum(cm)]\n",
        "  labels = [f'{v1}\\n{v2}' for v1, v2 in zip(group_counts, group_percentages)]\n",
        "  labels = np.array(labels).reshape(2,2)\n",
        "  sns.heatmap(cm, annot=labels, cmap='Blues', xticklabels=['Predicted Positive', 'Predicted Negative'], yticklabels=['Actual Positive', 'Actual Negative'], fmt='')\n",
        "  return"
      ],
      "metadata": {
        "id": "4spKBZ7ehSmA"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_test_set(model, iterator, criterion, device):\n",
        "  model.eval()\n",
        "\n",
        "  total_correct = 0\n",
        "  total_samples = 0\n",
        "\n",
        "  all_preds = []\n",
        "  all_labels = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "      for texts, labels, lengths in iterator:\n",
        "          texts = texts.to(device)\n",
        "          labels = labels.to(device)\n",
        "          lengths = lengths.to(device)\n",
        "\n",
        "          predictions = model(texts, lengths).squeeze(1)\n",
        "\n",
        "          # Cálculo da loss\n",
        "          loss = criterion(predictions, labels)\n",
        "\n",
        "          # Cálculo da acurácia\n",
        "          probs = torch.sigmoid(predictions)\n",
        "          predicted_classes = torch.round(probs)\n",
        "          correct = (predicted_classes == labels).sum().item()\n",
        "\n",
        "          total_correct += correct\n",
        "          total_samples += labels.size(0)\n",
        "\n",
        "          # Armazenamento das labels e previsões\n",
        "          all_preds.extend(predicted_classes.cpu().numpy())\n",
        "          all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "  avg_acc = total_correct / total_samples\n",
        "  return avg_acc, np.array(all_labels), np.array(all_preds)"
      ],
      "metadata": {
        "id": "5Q0R8RPvLJ8y"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avg_acc, y_true, y_pred = evaluate_test_set(model_scratch, test_loader, crit, device)"
      ],
      "metadata": {
        "id": "4Q6bU4LuNS83"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_metrics(y_true, y_pred)"
      ],
      "metadata": {
        "id": "ZYJbPBDciV5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(y_true, y_pred)"
      ],
      "metadata": {
        "id": "ei1DGYPgie94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avg_acc, y_true, y_pred = evaluate_test_set(model_glove, test_loader, crit, device)"
      ],
      "metadata": {
        "id": "diXBvrPyOykA"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_metrics(y_true, y_pred)"
      ],
      "metadata": {
        "id": "LDKX5JSGO2dw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(y_true, y_pred)"
      ],
      "metadata": {
        "id": "Ti-GtCb-O4L5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
